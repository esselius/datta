apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: pyspark-pi
spec:
  type: Python
  image: spark-job
  mainApplicationFile: local:///job.py
  sparkVersion: 3.5.0
  driver:
    cores: 1
    serviceAccount: spark
    env:
    - name: AWS_REGION
      value: us-west-1
    - name: AWS_ACCESS_KEY_ID
      value: minio
    - name: AWS_SECRET_ACCESS_KEY
      value: minio123
  executor:
    cores: 1
    env:
    - name: AWS_REGION
      value: us-west-1
    - name: AWS_ACCESS_KEY_ID
      value: minio
    - name: AWS_SECRET_ACCESS_KEY
      value: minio123
  deps:
    packages:
      - org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2
      - org.apache.iceberg:iceberg-aws-bundle:1.4.2
      - org.apache.hadoop:hadoop-aws:2.10.1
  sparkConf:
    spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
    spark.sql.catalog.hive_prod: org.apache.iceberg.spark.SparkCatalog
    spark.sql.catalog.hive_prod.type: hive
    spark.sql.catalog.hive_prod.uri: thrift://hive-metastore.hive:9083
    spark.sql.catalog.hive_prod.warehouse: s3://datalake/warehouse
    spark.sql.catalog.hive_prod.s3.endpoint: http://minio.minio:80
    spark.sql.catalog.hive_prod.s3.path-style-access: "true"
    spark.sql.catalog.hive_prod.io-impl: org.apache.iceberg.aws.s3.S3FileIO
    spark.jars.ivy: /tmp/ivy
    spark.hadoop.fs.s3a.aws.credentials.provider: org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
    spark.hadoop.fs.s3a.endpoint: http://minio.minio:80
    spark.hadoop.fs.s3a.path.style.access: "true"
